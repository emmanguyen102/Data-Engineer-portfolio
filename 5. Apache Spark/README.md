# Using PySpark to do data analysis

## Overview

The aim of this project is to utilize PySpark with connector MongoDB to do data analysis. 

## Step to step procedure
The whole process can be accessed [here](https://github.com/emmanguyen102/Data-Engineer-portfolio/blob/main/5.%20Apache%20Spark/Batch_processing_PySpark.ipynb).
1. Create Spark Session with connection to MongoDB.
2. Data preprocessing.
3. Data Analysis on single data frame and from joining data frames.

## Prerequisites>
- Jupyter Notebook
- Spark 3.3.0
- PySpark same version with Spark
- MongoDB
- MongoDB Compass


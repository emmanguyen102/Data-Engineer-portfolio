# Using PySpark to do data analysis

## Overview

The aim of this project is to utilize PySpark with connector MongoDB to do data analysis. 

## Step to step procedure
The whole process can be accessed through file [Batch_Processing_PySpark](https://github.com/emmanguyen102/Data-Engineer-portfolio/blob/main/5.%20Apache%20Spark/Batch_processing_PySpark.ipynb).
1. Create Spark Session with connection to MongoDB.
2. Data preprocessing.
3. Data Analysis on single data frame and from joining data frames.

## Prerequisites
- Jupyter Notebook
- Spark 3.3.0
- PySpark same version with Spark
- MongoDB
- MongoDB Compass

# More information
The blog post about the process of creating this project is written in [this](https://medium.com/p/e36c04f91071) blog post
